{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeced362",
   "metadata": {},
   "source": [
    "### Mike Ogrysko\n",
    "### CS 766 Information Retrieval and Natural Language Processing\n",
    "\n",
    "Parsing the IMDB movie reviews for sentiment\n",
    "- IMDB movie review data\n",
    "- Top 20 most frequent words in reviews grouped by sentiment\n",
    "- 20 top frequent bigrams in reviews grouped by sentiment\n",
    "- 20 top frequent bigrams, which are 'NN' POS tagged in reviews grouped by sentiment\n",
    "- 4-grams that have counts 2 or more in reviews grouped by sentiment\n",
    "- Probabilities of words that come after \"worst film ever\" and \"best movie ever\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2704c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import numpy as np\n",
    "import operator\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc303598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reviews loaded 50000\n",
      "Total sentiments loaded 50000\n"
     ]
    }
   ],
   "source": [
    "Reviews, Sentiments = [], []\n",
    "\n",
    "with open('movie_data.csv','r', encoding='utf8') as fin:\n",
    "    reader = csv.reader(fin, delimiter=',', quotechar='\"')\n",
    "    header = next(reader)\n",
    "    for i, line in enumerate(reader):\n",
    "        Reviews += [line[0]]\n",
    "        Sentiments +=[int(line[1])]\n",
    "\n",
    "N=len(Reviews)\n",
    "M=len(Sentiments)\n",
    "print('Total reviews loaded', N)\n",
    "print('Total sentiments loaded', M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c232b493",
   "metadata": {},
   "source": [
    "**Top 20 most frequent words in reviews grouped by sentiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d577677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combination of stop words and punctuations, also get rid of br\n",
    "stop_words = stopwords.words('english') + list(punctuation)\n",
    "stop_words_set = set(stop_words) | set(['br', 'The', 'This'])\n",
    "\n",
    "#develop tokenizer\n",
    "def tokenize(text):\n",
    "    terms = word_tokenize(text)\n",
    "    #filter stop words\n",
    "    terms = [w for w in terms if w not in stop_words_set and not w.isdigit()]\n",
    "    #regex for contractions and other special character strings\n",
    "    terms = [w for w in terms if not re.search(r'^\\W+|\\w\\'\\w+|\\'\\w+$', w)]\n",
    "    terms = [w for w in terms if not re.search(r'^[^a-z]+$', w)]\n",
    "    #regex for words two letters or less and numbers\n",
    "    terms = [w for w in terms if not re.search(r'^\\b\\w{1,2}\\b|(?<!\\S)\\d+(?!\\S)$', w)]\n",
    "    #lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #was passing get_wordnet_pos() into lemmatizer but stopped because of memory issues\n",
    "    #terms = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in terms]\n",
    "    terms = [lemmatizer.lemmatize(w, 'n') for w in terms]\n",
    "    return terms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "214af7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get bag0 list\n",
    "Bag0 = []\n",
    "for i, review in enumerate(Reviews):\n",
    "    if Sentiments[i] == 0:\n",
    "        Bag0 += [review]\n",
    "        \n",
    "#get bag1 list\n",
    "Bag1 = []\n",
    "for i, review in enumerate(Reviews):\n",
    "    if Sentiments[i] == 1:\n",
    "        Bag1 += [review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f683ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get term/review counts - _reviews = list, _pos = 0 or 1 (no or yes)\n",
    "def tokenize_dict(_reviews, _pos):\n",
    "    my_dict = defaultdict(int)\n",
    "    for review in _reviews:\n",
    "        if _pos == 1:\n",
    "            terms = set(nltk.pos_tag(tokenize(review)))\n",
    "        else:\n",
    "            terms = set(tokenize(review))\n",
    "        for term in terms:\n",
    "            my_dict[term] +=1\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fca2cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create count dictionaries for the bags - no POS\n",
    "vocab_counts_bag0 = tokenize_dict(Bag0, 0)\n",
    "vocab_counts_bag1 = tokenize_dict(Bag1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bded5666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the dictionaries and store the top 20\n",
    "sort_vocab_counts_bag0 = dict(sorted(vocab_counts_bag0.items(), key=lambda kv:kv[1],reverse=True)[:20])\n",
    "sort_vocab_counts_bag1 = dict(sorted(vocab_counts_bag1.items(), key=lambda kv:kv[1],reverse=True)[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1811ddd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment 0 - 20 most frequent\n",
      "17151 movie\n",
      "14096 film\n",
      "13424 one\n",
      "12300 like\n",
      "9540 time\n",
      "9461 would\n",
      "9244 good\n",
      "8975 even\n",
      "8693 make\n",
      "8566 get\n",
      "8355 bad\n",
      "8060 character\n",
      "7908 could\n",
      "7732 really\n",
      "7700 see\n",
      "7166 much\n",
      "6784 scene\n",
      "6704 story\n",
      "6645 thing\n",
      "6437 made\n"
     ]
    }
   ],
   "source": [
    "#print top 20 bag 0\n",
    "print(\"Sentiment 0 - 20 most frequent\")\n",
    "for i in sort_vocab_counts_bag0:\n",
    "    print(f\"{sort_vocab_counts_bag0[i]} {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e48b1d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment 1 - 20 most frequent\n",
      "14649 movie\n",
      "14533 film\n",
      "13513 one\n",
      "10283 like\n",
      "9636 time\n",
      "8872 good\n",
      "8454 see\n",
      "8278 story\n",
      "8128 character\n",
      "7962 great\n",
      "7737 make\n",
      "7490 get\n",
      "7449 would\n",
      "7197 well\n",
      "6929 really\n",
      "6693 also\n",
      "6453 much\n",
      "6208 way\n",
      "6145 even\n",
      "6121 scene\n"
     ]
    }
   ],
   "source": [
    "#print top 20 bag 1\n",
    "print(\"Sentiment 1 - 20 most frequent\")\n",
    "for i in sort_vocab_counts_bag1:\n",
    "    print(f\"{sort_vocab_counts_bag1[i]} {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682b420b",
   "metadata": {},
   "source": [
    "**20 top frequent bigrams in reviews grouped by sentiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "921d6a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grams_dict(_text, _n):\n",
    "    grams_dict_counts = defaultdict(int)\n",
    "    for review in _text:\n",
    "        terms = tokenize(review)\n",
    "        if len(terms) >= _n:\n",
    "            for i in range(len(terms)-_n+1):\n",
    "                gram_li = [_ for _ in terms[i:i+_n]]\n",
    "                gram = ' '.join(gram_li)\n",
    "                grams_dict_counts[gram] += 1\n",
    "    return grams_dict_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45e70585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create count dictionaries for the bags - no POS\n",
    "bigram_counts_bag0 = grams_dict(Bag0, 2)\n",
    "bigram_counts_bag1 = grams_dict(Bag1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37134826",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the dictionaries and store the top 20\n",
    "sort_bigram_counts_bag0 = dict(sorted(bigram_counts_bag0.items(), key=lambda kv:kv[1],reverse=True)[:20])\n",
    "sort_bigram_counts_bag1 = dict(sorted(bigram_counts_bag1.items(), key=lambda kv:kv[1],reverse=True)[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44619e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment 0 - 20 most frequent bigrams\n",
      "2127 look like\n",
      "1611 ever seen\n",
      "1383 special effect\n",
      "1363 waste time\n",
      "1252 movie ever\n",
      "1136 bad movie\n",
      "1057 main character\n",
      "1026 worst movie\n",
      "980 movie like\n",
      "949 horror movie\n",
      "937 much better\n",
      "867 year old\n",
      "866 one worst\n",
      "828 low budget\n",
      "787 make movie\n",
      "780 good movie\n",
      "748 horror film\n",
      "735 watch movie\n",
      "729 see movie\n",
      "710 bad guy\n"
     ]
    }
   ],
   "source": [
    "#print top 20 bag 0\n",
    "print(\"Sentiment 0 - 20 most frequent bigrams\")\n",
    "for i in sort_bigram_counts_bag0:\n",
    "    print(f\"{sort_bigram_counts_bag0[i]} {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2385c2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment 1 - 20 most frequent bigrams\n",
      "1488 one best\n",
      "927 ever seen\n",
      "893 first time\n",
      "832 even though\n",
      "821 New York\n",
      "818 main character\n",
      "765 special effect\n",
      "744 year old\n",
      "737 year ago\n",
      "732 see movie\n",
      "715 look like\n",
      "706 movie like\n",
      "678 good movie\n",
      "670 great movie\n",
      "635 film like\n",
      "604 movie ever\n",
      "585 horror film\n",
      "568 great film\n",
      "566 watch movie\n",
      "561 well done\n"
     ]
    }
   ],
   "source": [
    "#print top 20 bag 1\n",
    "print(\"Sentiment 1 - 20 most frequent bigrams\")\n",
    "for i in sort_bigram_counts_bag1:\n",
    "    print(f\"{sort_bigram_counts_bag1[i]} {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5402b66",
   "metadata": {},
   "source": [
    "**20 top frequent bigrams, which are 'NN' POS tagged in reviews grouped by sentiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f8fe469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grams_dict_NN(_text, _n):\n",
    "    grams_dict_counts = defaultdict(int)\n",
    "    for review in _text:\n",
    "        terms = nltk.pos_tag(tokenize(review))\n",
    "        if len(terms) >= _n:\n",
    "            for i in range(len(terms)-_n+1):\n",
    "                count = 0\n",
    "                gram_li, gram_pos = [], []\n",
    "                for term in terms[i:i+_n]:\n",
    "                    gram_li.append(term[0])\n",
    "                    gram_pos.append(term[1])\n",
    "                    if 'NN' in term[1]:\n",
    "                        count += 1\n",
    "                if count == _n:\n",
    "                    key=\"\"\n",
    "                    for j, k in enumerate(gram_li):\n",
    "                        key += k +\" (\"+ gram_pos[j]+\") \"\n",
    "                    grams_dict_counts[key.strip()] += 1\n",
    "    return grams_dict_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63fea5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create count dictionaries for the bags - wPOS\n",
    "bigramNN_counts_bag0 = grams_dict_NN(Bag0, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ea03799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create count dictionaries for the bags - wPOS\n",
    "bigramNN_counts_bag1 = grams_dict_NN(Bag1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f93f93ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the dictionaries and store the top 20\n",
    "sort_bigramNN_counts_bag0 = dict(sorted(bigramNN_counts_bag0.items(), key=lambda kv:kv[1],reverse=True)[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c66aeae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the dictionaries and store the top 20\n",
    "sort_bigramNN_counts_bag1 = dict(sorted(bigramNN_counts_bag1.items(), key=lambda kv:kv[1],reverse=True)[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abbc7183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment 0 - 20 most frequent bigrams w NN\n",
      "1252 waste (NN) time (NN)\n",
      "814 horror (NN) movie (NN)\n",
      "733 horror (NN) film (NN)\n",
      "478 story (NN) line (NN)\n",
      "460 watch (NN) movie (NN)\n",
      "436 New (NNP) York (NNP)\n",
      "432 part (NN) movie (NN)\n",
      "411 thing (NN) movie (NN)\n",
      "397 production (NN) value (NN)\n",
      "383 character (NN) development (NN)\n",
      "372 movie (NN) movie (NN)\n",
      "346 sex (NN) scene (NN)\n",
      "299 camera (NN) work (NN)\n",
      "293 action (NN) scene (NN)\n",
      "287 action (NN) movie (NN)\n",
      "284 time (NN) money (NN)\n",
      "273 watch (NN) film (NN)\n",
      "266 plot (NN) line (NN)\n",
      "261 film (NN) maker (NN)\n",
      "257 time (NN) movie (NN)\n"
     ]
    }
   ],
   "source": [
    "#print top 20 bag 0\n",
    "print(\"Sentiment 0 - 20 most frequent bigrams w NN\")\n",
    "for i in sort_bigramNN_counts_bag0:\n",
    "    print(f\"{sort_bigramNN_counts_bag0[i]} {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94c53ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment 1 - 20 most frequent bigrams w NN\n",
      "821 New (NNP) York (NNP)\n",
      "571 horror (NN) film (NN)\n",
      "454 story (NN) line (NN)\n",
      "413 horror (NN) movie (NN)\n",
      "349 watch (NN) movie (NN)\n",
      "338 World (NNP) War (NNP)\n",
      "284 movie (NN) movie (NN)\n",
      "266 watch (NN) film (NN)\n",
      "257 see (NN) film (NN)\n",
      "256 part (NN) movie (NN)\n",
      "255 movie (NN) time (NN)\n",
      "251 end (NN) film (NN)\n",
      "249 film (NN) film (NN)\n",
      "245 action (NN) movie (NN)\n",
      "245 movie (NN) watch (NN)\n",
      "241 point (NN) view (NN)\n",
      "237 film (NN) time (NN)\n",
      "236 production (NN) value (NN)\n",
      "231 part (NN) film (NN)\n",
      "231 Film (NNP) Festival (NNP)\n"
     ]
    }
   ],
   "source": [
    "#print top 20 bag 1\n",
    "print(\"Sentiment 1 - 20 most frequent bigrams w NN\")\n",
    "for i in sort_bigramNN_counts_bag1:\n",
    "    print(f\"{sort_bigramNN_counts_bag1[i]} {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1f0fda",
   "metadata": {},
   "source": [
    "**4-grams that have counts 2 or more in reviews grouped by sentiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bace30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate 4grams for Bag0\n",
    "bag0_4gram_dict = grams_dict(Bag0, 4)\n",
    "bag0_4gram_dict = {k:v for k, v in bag0_4gram_dict.items() if v >= 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ab57ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate 4grams for Bag1\n",
    "bag1_4gram_dict = grams_dict(Bag1, 4)\n",
    "bag1_4gram_dict = {k:v for k, v in bag1_4gram_dict.items() if v >= 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "438da328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the dictionaries and give the top 5\n",
    "sorted_bag0_4gram = sorted(bag0_4gram_dict.items(), key= lambda kv:kv[1], reverse=True)\n",
    "sorted_bag1_4gram = sorted(bag1_4gram_dict.items(), key= lambda kv:kv[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "960e05fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment 0 4grams - all\n",
      "\n",
      "392 worst movie ever seen\n",
      "215 one worst movie ever\n",
      "170 worst film ever seen\n",
      "114 worst movie ever made\n",
      "107 one worst film ever\n",
      "69 worst film ever made\n",
      "44 life never get back\n",
      "42 movie ever seen life\n",
      "28 movie seen long time\n",
      "28 one worst movie seen\n",
      "27 Plan From Outer Space\n",
      "25 could done better job\n",
      "24 movie complete waste time\n",
      "24 really wanted like movie\n",
      "23 trivialBoring trivialBoring trivialBoring trivialBoring\n",
      "22 One worst movie ever\n",
      "21 vote four. Title Brazil\n",
      "20 ever seen entire life\n",
      "19 left cutting room floor\n",
      "19 possibly worst film ever\n"
     ]
    }
   ],
   "source": [
    "print('Sentiment 0 4grams - all\\n')\n",
    "for i in sorted_bag0_4gram[:20]:\n",
    "    print(f\"{i[1]} {i[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb7f3ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment 1 4grams - all\n",
      "\n",
      "52 best movie ever seen\n",
      "47 one best movie ever\n",
      "37 Never Say Never Again\n",
      "34 one best movie seen\n",
      "29 vote eight. Title Brazil\n",
      "29 movie seen long time\n",
      "28 vote seven. Title Brazil\n",
      "28 funniest movie ever seen\n",
      "26 Best Years Our Lives\n",
      "26 Tony Hawk Pro Skater\n",
      "25 one best film ever\n",
      "25 one best film seen\n",
      "25 best movie ever made\n",
      "24 greatest show ever mad\n",
      "24 show ever mad full\n",
      "23 good guy bad guy\n",
      "23 ever mad full stop.OZ\n",
      "23 mad full stop.OZ greatest\n",
      "23 full stop.OZ greatest show\n",
      "23 stop.OZ greatest show ever\n"
     ]
    }
   ],
   "source": [
    "print('Sentiment 1 4grams - all\\n')\n",
    "for i in sorted_bag1_4gram[:20]:\n",
    "    print(f\"{i[1]} {i[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dad30d5",
   "metadata": {},
   "source": [
    "**Probabilities of words that come after \"worst film ever\" and \"best movie ever\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "754fb37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get full vocab dict\n",
    "vocab_4gram_dict = grams_dict(Reviews, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "947c5cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dictionary of 'worst film ever'\n",
    "vocab_4gram_dict_worst = {k:vocab_4gram_dict[k] for k in vocab_4gram_dict if 'worst film ever ' in k}\n",
    "sorted_vocab_4gram_dict_worst = dict( sorted(vocab_4gram_dict_worst.items(), key= lambda kv:kv[1], reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a33c067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 'worst film ever': 327\n",
      "\n",
      "Probabilities of 'worst film ever': \n",
      "\n",
      "worst film ever seen 0.523\n",
      "worst film ever made 0.223\n",
      "worst film ever seen. 0.018\n",
      "worst film ever misfortune 0.015\n",
      "worst film ever saw 0.012\n",
      "worst film ever see 0.009\n",
      "worst film ever created 0.009\n",
      "worst film ever paid 0.009\n",
      "worst film ever watched 0.006\n",
      "worst film ever viewed 0.006\n",
      "worst film ever displeasure 0.006\n",
      "worst film ever sat 0.006\n",
      "worst film ever wasted 0.006\n",
      "worst film ever made. 0.006\n",
      "worst film ever witnessed 0.006\n",
      "worst film ever acting 0.006\n",
      "worst film ever bad 0.003\n",
      "worst film ever well 0.003\n",
      "worst film ever conceived 0.003\n",
      "worst film ever Not 0.003\n",
      "worst film ever successful 0.003\n",
      "worst film ever Watchers 0.003\n",
      "worst film ever though 0.003\n",
      "worst film ever missfortune 0.003\n",
      "worst film ever encountered 0.003\n",
      "worst film ever mean 0.003\n",
      "worst film ever like 0.003\n",
      "worst film ever Hulk 0.003\n",
      "worst film ever generally 0.003\n",
      "worst film ever major 0.003\n",
      "worst film ever character 0.003\n",
      "worst film ever seen.And 0.003\n",
      "worst film ever time 0.003\n",
      "worst film ever cinematography 0.003\n",
      "worst film ever music 0.003\n",
      "worst film ever showed 0.003\n",
      "worst film ever certainly 0.003\n",
      "worst film ever ending 0.003\n",
      "worst film ever period 0.003\n",
      "worst film ever done 0.003\n",
      "worst film ever And 0.003\n",
      "worst film ever horror 0.003\n",
      "worst film ever supposed 0.003\n",
      "worst film ever appear 0.003\n",
      "worst film ever dramatic 0.003\n",
      "worst film ever say 0.003\n",
      "worst film ever starred 0.003\n",
      "worst film ever shot 0.003\n",
      "worst film ever When 0.003\n",
      "worst film ever likely 0.003\n",
      "worst film ever Now 0.003\n",
      "worst film ever look 0.003\n",
      "worst film ever hardly 0.003\n",
      "worst film ever Because 0.003\n",
      "worst film ever many 0.003\n",
      "worst film ever seen.I 0.003\n",
      "worst film ever win 0.003\n",
      "worst film ever bought 0.003\n",
      "worst film ever come 0.003\n"
     ]
    }
   ],
   "source": [
    "#calculate probabilities of worst film ever\n",
    "sum_worst = 0\n",
    "for k in sorted_vocab_4gram_dict_worst:\n",
    "    sum_worst += sorted_vocab_4gram_dict_worst[k]\n",
    "print(f\"Count 'worst film ever': {sum_worst}\\n\")\n",
    "\n",
    "sum_worst_prob = {}\n",
    "for k in sorted_vocab_4gram_dict_worst:\n",
    "    sum_worst_prob[k] = sorted_vocab_4gram_dict_worst[k]/sum_worst\n",
    "\n",
    "print(f\"Probabilities of 'worst film ever': \\n\")\n",
    "for i in sum_worst_prob:\n",
    "    print(f\"{i} {sum_worst_prob[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f04d862",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dictionary of 'best movie ever'\n",
    "vocab_4gram_dict_best = {k:vocab_4gram_dict[k] for k in vocab_4gram_dict if 'best movie ever ' in k and 'dumbest' not in k}\n",
    "sorted_vocab_4gram_dict_best = dict( sorted(vocab_4gram_dict_best.items(), key= lambda kv:kv[1], reverse=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a16973b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 'best movie ever': 145\n",
      "\n",
      "Probabilities of 'best movie ever': \n",
      "\n",
      "best movie ever seen 0.400\n",
      "best movie ever made 0.241\n",
      "best movie ever saw 0.028\n",
      "best movie ever seen. 0.028\n",
      "best movie ever watched 0.014\n",
      "best movie ever perfect 0.007\n",
      "best movie ever And 0.007\n",
      "best movie ever nice 0.007\n",
      "best movie ever saw.Overcoming 0.007\n",
      "best movie ever sitting 0.007\n",
      "best movie ever Mira 0.007\n",
      "best movie ever got 0.007\n",
      "best movie ever heart-stopping 0.007\n",
      "best movie ever make 0.007\n",
      "best movie ever put 0.007\n",
      "best movie ever bunch 0.007\n",
      "best movie ever hand 0.007\n",
      "best movie ever care 0.007\n",
      "best movie ever Holocaust 0.007\n",
      "best movie ever according 0.007\n",
      "best movie ever one 0.007\n",
      "best movie ever still 0.007\n",
      "best movie ever viewed. 0.007\n",
      "best movie ever get 0.007\n",
      "best movie ever Absolutely 0.007\n",
      "best movie ever funny 0.007\n",
      "best movie ever movie 0.007\n",
      "best movie ever watch 0.007\n",
      "best movie ever uncommon 0.007\n",
      "best movie ever Fire 0.007\n",
      "best movie ever opinion 0.007\n",
      "best movie ever idea 0.007\n",
      "best movie ever Why 0.007\n",
      "best movie ever Based 0.007\n",
      "best movie ever made.anyway.as 0.007\n",
      "best movie ever hit 0.007\n",
      "best movie ever blank 0.007\n",
      "best movie ever love 0.007\n",
      "best movie ever come 0.007\n",
      "best movie ever well 0.007\n",
      "best movie ever Ten 0.007\n",
      "best movie ever made. 0.007\n",
      "best movie ever Both 0.007\n",
      "best movie ever created 0.007\n",
      "best movie ever bro 0.007\n",
      "best movie ever lot 0.007\n",
      "best movie ever even 0.007\n"
     ]
    }
   ],
   "source": [
    "#calculate probabilities of best movie ever\n",
    "sum_best = 0\n",
    "for k in sorted_vocab_4gram_dict_best:\n",
    "    sum_best += sorted_vocab_4gram_dict_best[k]\n",
    "print(f\"Count 'best movie ever': {sum_best}\\n\")\n",
    "\n",
    "sum_best_prob = {}\n",
    "for k in sorted_vocab_4gram_dict_best:\n",
    "    sum_best_prob[k] = sorted_vocab_4gram_dict_best[k]/sum_best\n",
    "\n",
    "print(f\"Probabilities of 'best movie ever': \\n\")\n",
    "for i in sum_best_prob:\n",
    "    print(f\"{i} {sum_best_prob[i]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
