{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeced362",
   "metadata": {},
   "source": [
    "### Mike Ogrysko\n",
    "### CS 766 Information Retrieval and Natural Language Processing\n",
    "\n",
    "Parsing the IMDB movie reviews for sentiment\n",
    "- IMDB movie review data\n",
    "- Top 20 most frequent words in reviews grouped by sentiment\n",
    "- 20 top frequent bigrams in reviews grouped by sentiment\n",
    "- 20 top frequent bigrams, which are 'NN' POS tagged in reviews grouped by sentiment\n",
    "- 4-grams that have counts 2 or more in reviews grouped by sentiment\n",
    "- Probabilities of words that come after \"worst film ever\" and \"best movie ever\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2704c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import numpy as np\n",
    "import operator\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc303598",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reviews, Sentiments = [], []\n",
    "\n",
    "with open('../Datasets-20220907/movie_data.csv','r', encoding='utf8') as fin:\n",
    "    reader = csv.reader(fin, delimiter=',', quotechar='\"')\n",
    "    header = next(reader)\n",
    "    for i, line in enumerate(reader):\n",
    "        Reviews += [line[0]]\n",
    "        Sentiments +=[int(line[1])]\n",
    "\n",
    "N=len(Reviews)\n",
    "M=len(Sentiments)\n",
    "print('Total reviews loaded', N)\n",
    "print('Total sentiments loaded', M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c232b493",
   "metadata": {},
   "source": [
    "**Top 20 most frequent words in reviews grouped by sentiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d577677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combination of stop words and punctuations, also get rid of br\n",
    "stop_words = stopwords.words('english') + list(punctuation)\n",
    "stop_words_set = set(stop_words) | set(['br', 'The', 'This'])\n",
    "\n",
    "#develop tokenizer\n",
    "def tokenize(text):\n",
    "    terms = word_tokenize(text)\n",
    "    #filter stop words\n",
    "    terms = [w for w in terms if w not in stop_words_set and not w.isdigit()]\n",
    "    #regex for contractions and other special character strings\n",
    "    terms = [w for w in terms if not re.search(r'^\\W+|\\w\\'\\w+|\\'\\w+$', w)]\n",
    "    terms = [w for w in terms if not re.search(r'^[^a-z]+$', w)]\n",
    "    #regex for words two letters or less and numbers\n",
    "    terms = [w for w in terms if not re.search(r'^\\b\\w{1,2}\\b|(?<!\\S)\\d+(?!\\S)$', w)]\n",
    "    #lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #was passing get_wordnet_pos() into lemmatizer but stopped because of memory issues\n",
    "    #terms = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in terms]\n",
    "    terms = [lemmatizer.lemmatize(w, 'n') for w in terms]\n",
    "    return terms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214af7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get bag0 list\n",
    "Bag0 = []\n",
    "for i, review in enumerate(Reviews):\n",
    "    if Sentiments[i] == 0:\n",
    "        Bag0 += [review]\n",
    "        \n",
    "#get bag1 list\n",
    "Bag1 = []\n",
    "for i, review in enumerate(Reviews):\n",
    "    if Sentiments[i] == 1:\n",
    "        Bag1 += [review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f683ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get term/review counts - _reviews = list, _pos = 0 or 1 (no or yes)\n",
    "def tokenize_dict(_reviews, _pos):\n",
    "    my_dict = defaultdict(int)\n",
    "    for review in _reviews:\n",
    "        if _pos == 1:\n",
    "            terms = set(nltk.pos_tag(tokenize(review)))\n",
    "        else:\n",
    "            terms = set(tokenize(review))\n",
    "        for term in terms:\n",
    "            my_dict[term] +=1\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fca2cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create count dictionaries for the bags - no POS\n",
    "vocab_counts_bag0 = tokenize_dict(Bag0, 0)\n",
    "vocab_counts_bag1 = tokenize_dict(Bag1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bded5666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the dictionaries and store the top 20\n",
    "sort_vocab_counts_bag0 = dict(sorted(vocab_counts_bag0.items(), key=lambda kv:kv[1],reverse=True)[:20])\n",
    "sort_vocab_counts_bag1 = dict(sorted(vocab_counts_bag1.items(), key=lambda kv:kv[1],reverse=True)[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1811ddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print top 20 bag 0\n",
    "print(\"Sentiment 0 - 20 most frequent\")\n",
    "for i in sort_vocab_counts_bag0:\n",
    "    print(f\"{sort_vocab_counts_bag0[i]} {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48b1d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print top 20 bag 1\n",
    "print(\"Sentiment 1 - 20 most frequent\")\n",
    "for i in sort_vocab_counts_bag1:\n",
    "    print(f\"{sort_vocab_counts_bag1[i]} {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682b420b",
   "metadata": {},
   "source": [
    "**20 top frequent bigrams in reviews grouped by sentiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921d6a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grams_dict(_text, _n):\n",
    "    grams_dict_counts = defaultdict(int)\n",
    "    for review in _text:\n",
    "        terms = tokenize(review)\n",
    "        if len(terms) >= _n:\n",
    "            for i in range(len(terms)-_n+1):\n",
    "                gram_li = [_ for _ in terms[i:i+_n]]\n",
    "                gram = ' '.join(gram_li)\n",
    "                grams_dict_counts[gram] += 1\n",
    "    return grams_dict_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e70585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create count dictionaries for the bags - no POS\n",
    "bigram_counts_bag0 = grams_dict(Bag0, 2)\n",
    "bigram_counts_bag1 = grams_dict(Bag1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37134826",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the dictionaries and store the top 20\n",
    "sort_bigram_counts_bag0 = dict(sorted(bigram_counts_bag0.items(), key=lambda kv:kv[1],reverse=True)[:20])\n",
    "sort_bigram_counts_bag1 = dict(sorted(bigram_counts_bag1.items(), key=lambda kv:kv[1],reverse=True)[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44619e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print top 20 bag 0\n",
    "print(\"Sentiment 0 - 20 most frequent bigrams\")\n",
    "for i in sort_bigram_counts_bag0:\n",
    "    print(f\"{sort_bigram_counts_bag0[i]} {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2385c2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print top 20 bag 1\n",
    "print(\"Sentiment 1 - 20 most frequent bigrams\")\n",
    "for i in sort_bigram_counts_bag1:\n",
    "    print(f\"{sort_bigram_counts_bag1[i]} {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5402b66",
   "metadata": {},
   "source": [
    "**20 top frequent bigrams, which are 'NN' POS tagged in reviews grouped by sentiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8fe469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grams_dict_NN(_text, _n):\n",
    "    grams_dict_counts = defaultdict(int)\n",
    "    for review in _text:\n",
    "        terms = nltk.pos_tag(tokenize(review))\n",
    "        if len(terms) >= _n:\n",
    "            for i in range(len(terms)-_n+1):\n",
    "                count = 0\n",
    "                gram_li, gram_pos = [], []\n",
    "                for term in terms[i:i+_n]:\n",
    "                    gram_li.append(term[0])\n",
    "                    gram_pos.append(term[1])\n",
    "                    if 'NN' in term[1]:\n",
    "                        count += 1\n",
    "                if count == _n:\n",
    "                    key=\"\"\n",
    "                    for j, k in enumerate(gram_li):\n",
    "                        key += k +\" (\"+ gram_pos[j]+\") \"\n",
    "                    grams_dict_counts[key.strip()] += 1\n",
    "    return grams_dict_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fea5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create count dictionaries for the bags - wPOS\n",
    "bigramNN_counts_bag0 = grams_dict_NN(Bag0, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea03799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create count dictionaries for the bags - wPOS\n",
    "bigramNN_counts_bag1 = grams_dict_NN(Bag1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93f93ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the dictionaries and store the top 20\n",
    "sort_bigramNN_counts_bag0 = dict(sorted(bigramNN_counts_bag0.items(), key=lambda kv:kv[1],reverse=True)[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66aeae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the dictionaries and store the top 20\n",
    "sort_bigramNN_counts_bag1 = dict(sorted(bigramNN_counts_bag1.items(), key=lambda kv:kv[1],reverse=True)[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbc7183",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print top 20 bag 0\n",
    "print(\"Sentiment 0 - 20 most frequent bigrams w NN\")\n",
    "for i in sort_bigramNN_counts_bag0:\n",
    "    print(f\"{sort_bigramNN_counts_bag0[i]} {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c53ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print top 20 bag 1\n",
    "print(\"Sentiment 1 - 20 most frequent bigrams w NN\")\n",
    "for i in sort_bigramNN_counts_bag1:\n",
    "    print(f\"{sort_bigramNN_counts_bag1[i]} {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1f0fda",
   "metadata": {},
   "source": [
    "**4-grams that have counts 2 or more in reviews grouped by sentiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bace30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate 4grams for Bag0\n",
    "bag0_4gram_dict = grams_dict(Bag0, 4)\n",
    "bag0_4gram_dict = {k:v for k, v in bag0_4gram_dict.items() if v >= 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab57ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate 4grams for Bag1\n",
    "bag1_4gram_dict = grams_dict(Bag1, 4)\n",
    "bag1_4gram_dict = {k:v for k, v in bag1_4gram_dict.items() if v >= 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438da328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the dictionaries and give the top 5\n",
    "sorted_bag0_4gram = sorted(bag0_4gram_dict.items(), key= lambda kv:kv[1], reverse=True)\n",
    "sorted_bag1_4gram = sorted(bag1_4gram_dict.items(), key= lambda kv:kv[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960e05fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sentiment 0 4grams - all\\n')\n",
    "for i in sorted_bag0_4gram[:20]:\n",
    "    print(f\"{i[1]} {i[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7f3ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sentiment 1 4grams - all\\n')\n",
    "for i in sorted_bag1_4gram[:20]:\n",
    "    print(f\"{i[1]} {i[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dad30d5",
   "metadata": {},
   "source": [
    "**Probabilities of words that come after \"worst film ever\" and \"best movie ever\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754fb37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get full vocab dict\n",
    "vocab_4gram_dict = grams_dict(Reviews, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947c5cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dictionary of 'worst film ever'\n",
    "vocab_4gram_dict_worst = {k:vocab_4gram_dict[k] for k in vocab_4gram_dict if 'worst film ever ' in k}\n",
    "sorted_vocab_4gram_dict_worst = dict( sorted(vocab_4gram_dict_worst.items(), key= lambda kv:kv[1], reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a33c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate probabilities of worst film ever\n",
    "sum_worst = 0\n",
    "for k in sorted_vocab_4gram_dict_worst:\n",
    "    sum_worst += sorted_vocab_4gram_dict_worst[k]\n",
    "print(f\"Count 'worst film ever': {sum_worst}\\n\")\n",
    "\n",
    "sum_worst_prob = {}\n",
    "for k in sorted_vocab_4gram_dict_worst:\n",
    "    sum_worst_prob[k] = sorted_vocab_4gram_dict_worst[k]/sum_worst\n",
    "\n",
    "print(f\"Probabilities of 'worst film ever': \\n\")\n",
    "for i in sum_worst_prob:\n",
    "    print(f\"{i} {sum_worst_prob[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f04d862",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dictionary of 'best movie ever'\n",
    "vocab_4gram_dict_best = {k:vocab_4gram_dict[k] for k in vocab_4gram_dict if 'best movie ever ' in k and 'dumbest' not in k}\n",
    "sorted_vocab_4gram_dict_best = dict( sorted(vocab_4gram_dict_best.items(), key= lambda kv:kv[1], reverse=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a16973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate probabilities of best movie ever\n",
    "sum_best = 0\n",
    "for k in sorted_vocab_4gram_dict_best:\n",
    "    sum_best += sorted_vocab_4gram_dict_best[k]\n",
    "print(f\"Count 'best movie ever': {sum_best}\\n\")\n",
    "\n",
    "sum_best_prob = {}\n",
    "for k in sorted_vocab_4gram_dict_best:\n",
    "    sum_best_prob[k] = sorted_vocab_4gram_dict_best[k]/sum_best\n",
    "\n",
    "print(f\"Probabilities of 'best movie ever': \\n\")\n",
    "for i in sum_best_prob:\n",
    "    print(f\"{i} {sum_best_prob[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef191135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdd5f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0187b08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
